{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7 Reproducible Packaging and Release\n",
    "This is the final step in our Small Language Model lab series. Here we package our tuned model into a reusable, sharable form. The goal is to save adapters, tokenizer, and metadata in a clean structure and optionally push to Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 Stable installs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install -q --force-reinstall numpy==2.0.2 pandas==2.2.2 pyarrow==17.0.0\n",
    "%pip install -q datasets>=3.0.0 transformers>=4.41.0 peft>=0.11.0 accelerate>=0.29.0 sentencepiece>=0.1.99 tqdm>=4.66.0 bitsandbytes\n",
    "print(\"If imports fail, restart runtime and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Auto detect best adapters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "BASE = Path('/content/drive/MyDrive/slm-labs')\n",
    "assert BASE.exists(), f\"Missing {BASE}\"\n",
    "\n",
    "ADAPS=[]\n",
    "for r,ds,fs in os.walk(BASE):\n",
    "    if 'adapter_config.json' in fs:\n",
    "        ADAPS.append(Path(r))\n",
    "print('Adapters found:')\n",
    "for i,p in enumerate(ADAPS,1):\n",
    "    print(i,p)\n",
    "BEST_DIR = ADAPS[-1] if ADAPS else None\n",
    "print('Using BEST_DIR:', BEST_DIR)\n",
    "assert BEST_DIR and BEST_DIR.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Reload base model and adapters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "kw={}\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        kw=dict(device_map='auto', quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True), torch_dtype=torch.float16)\n",
    "    except Exception:\n",
    "        kw=dict(torch_dtype=torch.float16)\n",
    "else:\n",
    "    kw=dict(torch_dtype=torch.float32)\n",
    "\n",
    "Tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if Tok.pad_token is None:\n",
    "    Tok.pad_token = Tok.eos_token\n",
    "\n",
    "Base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, **kw)\n",
    "Tuned = PeftModel.from_pretrained(Base, str(BEST_DIR))\n",
    "Tuned.eval()\n",
    "print(\"Model with adapters ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Save adapters and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "REL_DIR = BASE / 'lab7_release'\n",
    "REL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Tok.save_pretrained(str(REL_DIR))\n",
    "Tuned.save_pretrained(str(REL_DIR))\n",
    "print(\"Saved adapters and tokenizer to\", REL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Write a model card"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "card = REL_DIR / 'README.md'\n",
    "with open(card,'w') as f:\n",
    "    f.write(\"# Domain Tuned Small Language Model\\n\")\n",
    "    f.write(\"This model was fine tuned with LoRA adapters as part of the Lab 1â€“7 SLM training series.\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"## Base Model\\n\")\n",
    "    f.write(\"HuggingFaceH4/zephyr-7b-beta\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"## Training Data\\n\")\n",
    "    f.write(\"Domain text from ncbi/Open-Patients, prepared in Lab 3.\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"## Method\\n\")\n",
    "    f.write(\"LoRA fine tuning with Unsloth, adapters attached in Lab 4, optimized in Lab 5, and evaluated in Lab 6.\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"## Intended Use\\n\")\n",
    "    f.write(\"For experimentation and research. Not for clinical or production use without further validation.\\n\")\n",
    "print(\"Model card saved to\", card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 Optional push to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# To push to Hugging Face Hub, first log in with your token:\n",
    "# from huggingface_hub import login\n",
    "# login(token='hf_your_token_here')\n",
    "\n",
    "# Then run:\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Tok.push_to_hub('your-username/your-model-name')\n",
    "# Tuned.push_to_hub('your-username/your-model-name')\n",
    "# print(\"Pushed to Hugging Face Hub.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
