{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 – Evaluation and Comparison\n",
    "Compare base vs tuned with quick perplexity and side-by-side generations, then save a brief report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 0. Stable installs"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --force-reinstall numpy==2.0.2 pandas==2.2.2 pyarrow==17.0.0\n",
    "%pip install -q datasets>=3.0.0 transformers>=4.41.0 peft>=0.11.0 accelerate>=0.29.0 sentencepiece>=0.1.99 tqdm>=4.66.0 bitsandbytes\n",
    "print('If imports fail, use Runtime → Restart runtime and re-run this cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 1. Auto-detect dataset and adapters"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "BASE = Path('/content/drive/MyDrive/slm-labs')\n",
    "assert BASE.exists(), f'Missing {BASE}. Create it or change BASE.'\n",
    "\n",
    "DSETS=[]\n",
    "for r,ds,fs in os.walk(BASE):\n",
    "    if 'dataset_info.json' in fs:\n",
    "        DSETS.append(Path(r))\n",
    "print('Datasets found:')\n",
    "for i,p in enumerate(DSETS,1):\n",
    "    print(i,p)\n",
    "DATA_DIR = DSETS[0] if DSETS else None\n",
    "print('Using DATA_DIR:', DATA_DIR)\n",
    "\n",
    "ADAPS=[]\n",
    "for r,ds,fs in os.walk(BASE):\n",
    "    if 'adapter_config.json' in fs:\n",
    "        ADAPS.append(Path(r))\n",
    "print('Adapters found:')\n",
    "for i,p in enumerate(ADAPS,1):\n",
    "    print(i,p)\n",
    "BEST_DIR = ADAPS[0] if ADAPS else None\n",
    "print('Using BEST_DIR:', BEST_DIR)\n",
    "\n",
    "assert DATA_DIR and DATA_DIR.exists()\n",
    "assert BEST_DIR and BEST_DIR.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 2. Load eval split"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "DS = load_from_disk(str(DATA_DIR))\n",
    "val = DS.get('validation') or DS.get('test')\n",
    "if val is None:\n",
    "    val = DS['train'].select(range(min(200, len(DS['train']))))\n",
    "print('Eval samples:', len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 3. Load base and tuned models"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL='HuggingFaceH4/zephyr-7b-beta'\n",
    "kw={}\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        kw=dict(device_map='auto', quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True), torch_dtype=torch.float16)\n",
    "    except Exception:\n",
    "        kw=dict(torch_dtype=torch.float16)\n",
    "else:\n",
    "    kw=dict(torch_dtype=torch.float32)\n",
    "Tok=AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "Base=AutoModelForCausalLM.from_pretrained(BASE_MODEL, **kw)\n",
    "if Tok.pad_token is None: Tok.pad_token=Tok.eos_token\n",
    "Tuned=PeftModel.from_pretrained(Base, str(BEST_DIR))\n",
    "Tuned.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 4. Quick perplexity"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "sub = val.select(range(min(512, len(val))))\n",
    "def ppl(model, tok, ds):\n",
    "    model.eval(); L = DataLoader(ds, batch_size=2, shuffle=False, collate_fn=default_data_collator)\n",
    "    tot=0; toks=0\n",
    "    with torch.no_grad():\n",
    "        for b in L:\n",
    "            b={k:v.to(model.device) for k,v in b.items() if hasattr(v,'to')}\n",
    "            out=model(**b, labels=b.get('input_ids'))\n",
    "            loss=out.loss\n",
    "            tot+=loss.item()*b['input_ids'].numel(); toks+=b['input_ids'].numel()\n",
    "    return math.exp(tot/max(1,toks))\n",
    "\n",
    "base_ppl=ppl(Base,Tok,sub); tuned_ppl=ppl(Tuned,Tok,sub)\n",
    "print('Base perplexity:',base_ppl,'Tuned perplexity:',tuned_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 5. Side-by-side generations"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts=[\n",
    " 'Draft a concise cardiology discharge summary for a patient treated for acute coronary syndrome.',\n",
    " 'Explain the difference between type 1 and type 2 diabetes in plain language for a patient handout.',\n",
    " 'Summarize key risk factors for stroke in three bullet points.'\n",
    "]\n",
    "CFG=dict(max_new_tokens=200, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=Tok.eos_token_id)\n",
    "def gen(m,t,p):\n",
    "    x=t(p, return_tensors='pt').to(m.device)\n",
    "    with torch.no_grad():\n",
    "        y=m.generate(**x,**CFG)\n",
    "    return t.decode(y[0], skip_special_tokens=True)\n",
    "\n",
    "for p in prompts:\n",
    "    print('\\nPrompt:',p)\n",
    "    print('\\nBase:\\n',gen(Base,Tok,p))\n",
    "    print('\\nTuned:\\n',gen(Tuned,Tok,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 6. Save a brief report to Drive"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "R=Path('/content/drive/MyDrive/slm-labs/lab6_report'); R.mkdir(parents=True, exist_ok=True)\n",
    "with open(R/'summary.txt','w') as f:\n",
    "    f.write('Lab 6 – Evaluation and Comparison\\n')\n",
    "    f.write(f'Base perplexity: {base_ppl:.3f}\\nTuned perplexity: {tuned_ppl:.3f}\\n\\n')\n",
    "    for i,p in enumerate(prompts,1):\n",
    "        f.write(f'Prompt {i}: {p}\\n')\n",
    "        f.write('Base\\n-----\\n'); f.write(gen(Base, Tok, p)+'\\n\\n')\n",
    "        f.write('Tuned\\n-----\\n'); f.write(gen(Tuned, Tok, p)+'\\n\\n')\n",
    "print('Saved report to /content/drive/MyDrive/slm-labs/lab6_report/summary.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
