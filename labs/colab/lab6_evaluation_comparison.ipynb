{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 – Evaluation and Comparison\n",
    "**Part 6 of the 7 Lab Hands-On SLM Training Series**\n",
    "\n",
    "In this lab, you will evaluate your tuned LoRA model against the original base model. We will compute a quick perplexity estimate on a validation split and run side-by-side qualitative prompts to compare outputs. Results and a brief report will be saved to Google Drive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Stable installs for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --force-reinstall \"numpy==2.0.2\" \"pandas==2.2.2\" \"pyarrow==17.0.0\"\n",
    "%pip install -q \"datasets>=3.0.0\" \"transformers>=4.41.0\" \"peft>=0.11.0\" \"accelerate>=0.29.0\" \"sentencepiece>=0.1.99\" \"tqdm>=4.66.0\" bitsandbytes\n",
    "import importlib\n",
    "for m in [\"numpy\",\"pandas\",\"pyarrow\",\"datasets\",\"transformers\",\"peft\",\"accelerate\",\"sentencepiece\",\"tqdm\"]:\n",
    "    mod = importlib.import_module(m)\n",
    "    print(m, getattr(mod, '__version__', 'unknown'))\n",
    "print('If imports fail, go to Runtime → Restart runtime, then re-run this cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Mount Google Drive and set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Dataset prepared in Lab 3\n",
    "DATA_DIR = Path(\"/content/drive/MyDrive/slm-labs/lab3_tokenized\")\n",
    "\n",
    "# Lab 5 results folder containing best adapters\n",
    "L5_DIR = Path(\"/content/drive/MyDrive/slm-labs/lab5_results\")\n",
    "\n",
    "# Auto-pick the most recent 'best_*' directory, otherwise set BEST_DIR manually\n",
    "best_dirs = sorted([p for p in L5_DIR.glob('best_*') if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "BEST_DIR = best_dirs[0] if best_dirs else None\n",
    "print(\"Selected BEST_DIR:\", BEST_DIR)\n",
    "\n",
    "assert DATA_DIR.exists(), f\"Dataset folder not found: {DATA_DIR}\"\n",
    "assert BEST_DIR is not None and BEST_DIR.exists(), \"No best_* adapters found in lab5_results. Run Lab 5 first or set BEST_DIR manually.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Load the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "ds = load_from_disk(str(DATA_DIR))\n",
    "print(ds)\n",
    "\n",
    "if 'validation' in ds:\n",
    "    eval_ds = ds['validation']\n",
    "elif 'test' in ds:\n",
    "    eval_ds = ds['test']\n",
    "else:\n",
    "    # If no split, take a small slice of train\n",
    "    eval_ds = ds['train'].select(range(min(200, len(ds['train']))))\n",
    "print('Eval samples:', len(eval_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Load base model and apply best LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"  # change if desired\n",
    "\n",
    "def load_base(name):\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    kwargs = {}\n",
    "    if use_gpu:\n",
    "        try:\n",
    "            quant = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4',\n",
    "                                       bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True)\n",
    "            kwargs.update(dict(device_map='auto', quantization_config=quant, torch_dtype=torch.float16))\n",
    "        except Exception:\n",
    "            kwargs.update(dict(torch_dtype=torch.float16))\n",
    "    else:\n",
    "        kwargs.update(dict(torch_dtype=torch.float32))\n",
    "    tok = AutoTokenizer.from_pretrained(name, use_fast=True)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(name, **kwargs)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok, mdl\n",
    "\n",
    "tokenizer_base, model_base = load_base(BASE_MODEL)\n",
    "tokenizer_tuned = tokenizer_base\n",
    "model_tuned = PeftModel.from_pretrained(model_base, str(BEST_DIR))\n",
    "model_tuned.eval()\n",
    "print('Loaded base and tuned models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Quantitative evaluation with perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "MAX_EXAMPLES = 512\n",
    "subset = eval_ds.select(range(min(MAX_EXAMPLES, len(eval_ds))))\n",
    "\n",
    "def perplexity(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=default_data_collator)\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            outputs = model(**batch, labels=batch.get('input_ids'))\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * batch['input_ids'].numel()\n",
    "            total_tokens += batch['input_ids'].numel()\n",
    "    avg_nll = total_loss / max(1, total_tokens)\n",
    "    return math.exp(avg_nll)\n",
    "\n",
    "ppl_base = perplexity(model_base, tokenizer_base, subset)\n",
    "ppl_tuned = perplexity(model_tuned, tokenizer_tuned, subset)\n",
    "print(f\"Base perplexity:  {ppl_base:.3f}\")\n",
    "print(f\"Tuned perplexity: {ppl_tuned:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Qualitative prompts side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Draft a concise cardiology discharge summary for a patient treated for acute coronary syndrome.\",\n",
    "    \"Explain the difference between type 1 and type 2 diabetes in plain language for a patient handout.\",\n",
    "    \"Summarize key risk factors for stroke in three bullet points.\",\n",
    "]\n",
    "\n",
    "gen_cfg = dict(max_new_tokens=200, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=tokenizer_base.eos_token_id)\n",
    "\n",
    "def generate_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, **gen_cfg)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "for i, p in enumerate(prompts, 1):\n",
    "    print(f\"\\nPrompt {i}: {p}\")\n",
    "    base_out = generate_text(model_base, tokenizer_base, p)\n",
    "    tuned_out = generate_text(model_tuned, tokenizer_tuned, p)\n",
    "    print(\"\\nBase model\\n\")\n",
    "    print(base_out)\n",
    "    print(\"\\nTuned model\\n\")\n",
    "    print(tuned_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Save results and a brief report to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dir = Path(\"/content/drive/MyDrive/slm-labs/lab6_report\")\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "report_path = report_dir / \"summary.txt\"\n",
    "\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(\"Lab 6 – Evaluation and Comparison\\n\")\n",
    "    f.write(f\"Base model: {BASE_MODEL}\\n\")\n",
    "    f.write(f\"Best adapters: {BEST_DIR}\\n\\n\")\n",
    "    f.write(f\"Base perplexity:  {ppl_base:.3f}\\n\")\n",
    "    f.write(f\"Tuned perplexity: {ppl_tuned:.3f}\\n\\n\")\n",
    "    for i, p in enumerate(prompts, 1):\n",
    "        f.write(f\"Prompt {i}: {p}\\n\")\n",
    "        f.write(\"Base model\\n\")\n",
    "        f.write(\"-----\\n\")\n",
    "        f.write(generate_text(model_base, tokenizer_base, p) + \"\\n\\n\")\n",
    "        f.write(\"Tuned model\\n\")\n",
    "        f.write(\"-----\\n\")\n",
    "        f.write(generate_text(model_tuned, tokenizer_tuned, p) + \"\\n\\n\")\n",
    "\n",
    "print(\"Saved report to\", report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Next steps\n",
    "Extend evaluation with task specific metrics if you have labeled data, such as accuracy or F1 for classification tasks, or BLEU and ROUGE for summarization. Consider building a small human evaluation rubric for clarity, correctness, and tone, and collect a few ratings to validate gains seen in perplexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
