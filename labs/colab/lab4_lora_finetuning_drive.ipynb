{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4ce046",
   "metadata": {},
   "source": [
    "# Lab 4 – LoRA Fine-Tuning\n",
    "**Part 4 of the 7 Lab Hands-On SLM Training Series**\n",
    "\n",
    "In this lab, we move from preparing our dataset to teaching the Small Language Model (SLM) how to adapt to our domain. We do this using **LoRA (Low-Rank Adaptation)**, a parameter-efficient fine-tuning technique supported by the `unsloth` library.\n",
    "\n",
    "By the end of this lab you will have:\n",
    "- Attached LoRA adapters to a base model\n",
    "- Run a short domain-adaptive training loop\n",
    "- Verified that the model is adapting to your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574e7ea",
   "metadata": {},
   "source": [
    "## Step 0. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q unsloth transformers datasets accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8345476",
   "metadata": {},
   "source": [
    "## Step 1. Load the prepared dataset from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c08aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load the tokenized dataset created in Lab 3 (saved to Google Drive)\n",
    "DATA_DIR = \"/content/drive/MyDrive/slm-labs/lab3_tokenized\"\n",
    "dataset = load_from_disk(DATA_DIR)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284156ba",
   "metadata": {},
   "source": [
    "## Step 2. Load the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"  # replace with your base SLM if preferred\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=1024,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model and tokenizer ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7edc3",
   "metadata": {},
   "source": [
    "## Step 3. Attach LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5752805",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "print(\"LoRA adapters attached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0feb3",
   "metadata": {},
   "source": [
    "## Step 4. Fine-tune with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ba951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import UnslothTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=50,  # keep small for demo\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    output_dir=\"./outputs\",\n",
    ")\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset.get(\"test\"),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59330906",
   "metadata": {},
   "source": [
    "## Step 5. Save LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba23991",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./lora_adapters\")\n",
    "tokenizer.save_pretrained(\"./lora_adapters\")\n",
    "print(\"LoRA adapters saved to ./lora_adapters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ad733",
   "metadata": {},
   "source": [
    "### Wrap-up\n",
    "In this lab you have:\n",
    "- Loaded your domain dataset from Lab 3 (via Google Drive)\n",
    "- Attached LoRA adapters to your base model\n",
    "- Run a short fine-tuning loop\n",
    "- Saved the resulting adapters\n",
    "\n",
    "These adapters can now be reapplied to the base model any time you want, making fine-tuning lightweight and reusable.\n",
    "\n",
    "Next up: **Lab 5 – Hyperparameter Tuning and Optimization**."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
