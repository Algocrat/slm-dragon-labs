{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Algocrat/slm-dragon-labs/blob/main/lab3_fresh_data_loading_tokenization_revised_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3fc949e",
      "metadata": {
        "id": "c3fc949e"
      },
      "source": [
        "# Lab 3 â€“ Data Loading and Tokenization\n",
        "**Part 3 of the 7 Lab Hands-On SLM Training Series**\n",
        "\n",
        "This notebook downloads the `ncbi/Open-Patients` dataset, performs basic cleaning and sanity checks, detects the text field automatically, and prepares tokenized chunks for causal language modeling (CLM). It saves a tokenized dataset to disk for use in Lab 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ao4XtC2xepx"
      },
      "source": [
        "### Note on dataset access and licensing\n",
        "The `ncbi/Open-Patients` dataset is publicly available on the Hugging Face Hub under CC-BY-SA 4.0. No authentication is required to download. Please provide attribution if you reuse the data, and ensure your use complies with the license and any applicable privacy rules."
      ],
      "id": "7Ao4XtC2xepx"
    },
    {
      "cell_type": "markdown",
      "id": "3c0e7425",
      "metadata": {
        "id": "3c0e7425"
      },
      "source": [
        "## Step 0. Install dependencies (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "02211130",
      "metadata": {
        "id": "02211130",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "398d878b-8647-444a-ba37-0a1c870cc95a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mDependencies OK\n"
          ]
        }
      ],
      "source": [
        "!pip -q install --upgrade datasets transformers sentencepiece pyarrow tqdm > /dev/null\n",
        "import importlib\n",
        "for m in [\"datasets\", \"transformers\", \"sentencepiece\", \"pyarrow\", \"tqdm\"]:\n",
        "    importlib.import_module(m)\n",
        "print(\"Dependencies OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b98dd8f",
      "metadata": {
        "id": "5b98dd8f"
      },
      "source": [
        "## Step 1. Download dataset: `ncbi/Open-Patients`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "35ae503e",
      "metadata": {
        "id": "35ae503e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376,
          "referenced_widgets": [
            "b05ba6844882440c95a1d2c48972c595",
            "ad47f289d9d54504bfce483739e47117",
            "e6d3147027414285893f2224af0bf0e6",
            "53242fb15c9640de82a60224855aedd1",
            "5c5f5457374a4c2faaa9a390cb0dec82",
            "48364dc9e66b4a269a85973de4c81c3e",
            "3b5abc4c38d540fab40a23ff6d6acc78",
            "fcbe3af26c0f40d89311ad21ef74fa5d",
            "7edb07dd8b554bc9b01c63de84656fc2",
            "d8df89fa6810486b9d0dcd8b771a64cd",
            "709623a4e5f846c99fc78d442ae7ec4b",
            "d6ca361d560640b7b22f41ef7a12d36b",
            "f010d427854b4f19b69cdd9c3f8b9361",
            "8d806bc1bad04a8db637eccf6d20c200",
            "616c8d7e7ad64975a8e25b87b4a5606d",
            "bbbd384bea244c85b57efc8945fc41e1",
            "66bdbba386a94fbfb277ae59cd2ea05a",
            "a721e80240d14a2a927e3344ec151358",
            "c8669aabd8ce453aa892981cf95c263a",
            "ae3a6a8103f940b8863516abb2c90151",
            "6f872727e3d646d49466e9120f637a40",
            "8a5ed253656a4c19bc6a8389582b912e",
            "19979eb6229e4e079de9301a34cab55d",
            "8d8147523aaf4e3ca76835cbb8ff9721",
            "485fc31091dd4cd8b626880f87642485",
            "3c0198b3bfab43b8bf4db4aa5f8d9291",
            "2d74f02378924e7292e4e1b4f04ef07f",
            "0b49bcb6bd6b4538b646b0a1a62e25de",
            "c1a807df16504b349c5c4a5a99950a57",
            "46e8b1c89a9740feb0d34a2488bf93b3",
            "13595551852e4b21843ae23ea8e938ff",
            "4039eae2db2b4b2d8f61e3312ec7c92d",
            "e0467686d1ef4c8f8a7c4af0697dfbe5"
          ]
        },
        "outputId": "4e33f998-747b-4e59-887e-8184f7fe0dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b05ba6844882440c95a1d2c48972c595"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Open-Patients.jsonl:   0%|          | 0.00/482M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6ca361d560640b7b22f41ef7a12d36b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/180142 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19979eb6229e4e079de9301a34cab55d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['_id', 'description'],\n",
            "        num_rows: 180142\n",
            "    })\n",
            "})\n",
            "Example record:\n",
            "{'_id': 'trec-cds-2014-1', 'description': 'A 58-year-old African-American woman presents to the ER with episodic pressing/burning anterior chest pain that began two days earlier for the first time in her life. The pain started while she was walking, radiates to the back, and is accompanied by nausea, diaphoresis and mild dyspnea, but is not increased on inspiration. The latest episode of pain ended half an hour prior to her arrival. She is known to have hypertension and obesity. She denies smoking, diabetes, hypercholesterolemia, or a family history of heart disease. She currently takes no medications. Physical examination is normal. The EKG shows nonspecific changes.'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Public dataset under CC-BY-SA 4.0; no authentication required\n",
        "dataset = load_dataset(\"ncbi/Open-Patients\")\n",
        "print(dataset)\n",
        "print(\"Example record:\")\n",
        "first_split = list(dataset.keys())[0]\n",
        "print(dataset[first_split][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "195c8c27",
      "metadata": {
        "id": "195c8c27"
      },
      "source": [
        "## Step 1.1 Clean and sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "abe5cf97",
      "metadata": {
        "id": "abe5cf97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "112bb5b2233c43ae92124bf8d992f665",
            "b29f4b29c7e04939b4dc5808f23541cf",
            "da45ceb1ad9d4d84bca128ab2c7f8715",
            "ca11812be63a4c9d84a5b9f7ad0c89b7",
            "8bd25df2f0844b8ab7638898818b5e83",
            "3e4fbd4eba764b2fa4ba1af730195479",
            "75777f0bcd5e4380be208061c605ec0a",
            "664825eea5114e08803b848bdcb647b5",
            "98581430ba84420b860072c7dd5f8bec",
            "d3c8e0fcbb5c4428be7550ea214ce3d9",
            "72678c9038f24b8195c9f512b47b57a6",
            "b6936fcc9ff243d4b47ca4424233dcb4",
            "ae741d74ae0847f180f041ba7ab3a22f",
            "005da489a1564424be1f33e1eea63258",
            "9155bc676a45475394f75fe850dc4a1a",
            "4fdfedf7415d4fcea650b2335ad8f461",
            "566a3d8e9c6e4a35b1e6fc4117d85298",
            "f460a87a3a9c4ecbbbf47276247dbb07",
            "8286b9e708a947f499fa9a8b52dcaa66",
            "cbcf3d7cc83f475bbcb3bf998b8153dc",
            "725f3f4e59d84138a161d0b352b168bc",
            "01eb3cf7539c4520b40045beee6d2b02"
          ]
        },
        "outputId": "b4dff1ff-3699-4a46-8af7-889e3334b928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using text field: description\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Cleaning train:   0%|          | 0/180142 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "112bb5b2233c43ae92124bf8d992f665"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filtering train:   0%|          | 0/180142 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6936fcc9ff243d4b47ca4424233dcb4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deduped train: 180140 -> 180138\n",
            "train: n=180138 mean=2614.0 p50=2353 p90=4633 p99=8081\n"
          ]
        }
      ],
      "source": [
        "import re, unicodedata\n",
        "from datasets import DatasetDict\n",
        "import numpy as np\n",
        "\n",
        "TEXT_FIELD_CANDIDATES = [\"text\", \"content\", \"description\", \"body\", \"note\"]\n",
        "sample_split = list(dataset.keys())[0]\n",
        "sample_item = dataset[sample_split][0]\n",
        "text_field = None\n",
        "for k in TEXT_FIELD_CANDIDATES:\n",
        "    if k in sample_item and isinstance(sample_item[k], str):\n",
        "        text_field = k\n",
        "        break\n",
        "if text_field is None:\n",
        "    for k, v in sample_item.items():\n",
        "        if isinstance(v, str):\n",
        "            text_field = k\n",
        "            break\n",
        "print(\"Using text field:\", text_field)\n",
        "\n",
        "KEEP_ASCII_ONLY = False\n",
        "MIN_LEN_CHARS = 10\n",
        "MAX_LEN_CHARS = 50000\n",
        "\n",
        "def basic_clean(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    if KEEP_ASCII_ONLY:\n",
        "        s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    return s\n",
        "\n",
        "def map_clean(example):\n",
        "    t = example.get(text_field, \"\")\n",
        "    t = basic_clean(t)\n",
        "    example[text_field] = t\n",
        "    example[\"len_chars\"] = len(t)\n",
        "    return example\n",
        "\n",
        "def is_valid(example):\n",
        "    ln = example[\"len_chars\"]\n",
        "    return (ln >= MIN_LEN_CHARS) and (ln <= MAX_LEN_CHARS)\n",
        "\n",
        "cleaned = DatasetDict()\n",
        "for split in dataset.keys():\n",
        "    cleaned_split = dataset[split].map(map_clean, desc=f\"Cleaning {split}\")\n",
        "    cleaned_split = cleaned_split.filter(is_valid, desc=f\"Filtering {split}\")\n",
        "    cleaned[split] = cleaned_split\n",
        "\n",
        "def dedupe_exact(ds, key):\n",
        "    seen = set()\n",
        "    idxs = []\n",
        "    for i, s in enumerate(ds[key]):\n",
        "        if s not in seen:\n",
        "            idxs.append(i)\n",
        "            seen.add(s)\n",
        "    return ds.select(idxs)\n",
        "\n",
        "for split in list(cleaned.keys()):\n",
        "    before = len(cleaned[split])\n",
        "    cleaned[split] = dedupe_exact(cleaned[split], text_field)\n",
        "    after = len(cleaned[split])\n",
        "    if after != before:\n",
        "        print(f\"Deduped {split}: {before} -> {after}\")\n",
        "\n",
        "for split in cleaned.keys():\n",
        "    arr = np.array(cleaned[split][\"len_chars\"])\n",
        "    if arr.size:\n",
        "        print(f\"{split}: n={arr.size} mean={arr.mean():.1f} p50={np.percentile(arr,50):.0f} \"\n",
        "              f\"p90={np.percentile(arr,90):.0f} p99={np.percentile(arr,99):.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e32de55",
      "metadata": {
        "id": "5e32de55"
      },
      "source": [
        "## Step 2. Initialize tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bd69399c",
      "metadata": {
        "id": "bd69399c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "d1ae4f511d4b413b938bd84a5d1af7b7",
            "9a088539fd3643c98027fe9bc025693a",
            "60708595d167427eaa94219c8417fc7e",
            "fb883875072c45df9c234f690be6dc41",
            "b4560fd0648f44b8984413d751d44505",
            "54df3366e2544fa099b50a7cd08dc8fb",
            "def11876abdc47fbb4d1d553580450c3",
            "a090b746622943e3ab75eef0f811a4d4",
            "7d3e4e1b0dc444ce8f42dbcfbec6194a",
            "a1c1541589ed446daec296a51c8d39d2",
            "e8a04b0b9da84c82963945a7794a0640",
            "a887123175bb4ff587f220243b873edb",
            "5ee3085a5b464384884b1ee4a5b4b5f3",
            "c84bf39ddd4349a1a32fc36f59c45f8e",
            "b73b5a6b55c4485d9345c731fb9ccd17",
            "2a9d656ea4254b3392cc01c7363e3f03",
            "90432ca36fac4f6ca9e0f64940aabd62",
            "17c4de80f57b424380ddc4dc895d6715",
            "5dbeb81bca094ee099f3a86146cd75b8",
            "ca03f1123b9940d1a03768cf8375904d",
            "78858715100d46f68e18e6d8855f82a6",
            "416923a250304becbca8d364ba243bac",
            "3f9aaa2486954755b0501a8608673ede",
            "379ee322431f47e2b8cd5cf34c8b203a",
            "cb4d2b78ba6f4f66a1fd25f1c16eb9bb",
            "f40c90cc7aed4ab89fa6dcaecb2ce60c",
            "0a07a6773cc248c1a81bc6d198ff2a53",
            "e8ed49a6ace84c808b08f49e87cb9358",
            "7fd47ea052754662807bdc401f107c8b",
            "f1f94290f85d4021a9e8f3eb8b0381d1",
            "84fbb4b53a664a8ab45bda5ec24e884c",
            "8bdca0f51b4e43838e7c94ae26053d1e",
            "c1d21293266148c39c5f272d19786576",
            "c3f7bc00997341e9b816073d72f018e2",
            "122eefcadd2b4c7e97b6792a5f96f0ae",
            "3f24a4cd63cb42eb9d544b346c506baf",
            "991cf4943c5a49919728fa41e3856301",
            "a96cf0abe7ac4d4c842a937f987400e1",
            "b98982c0b1494605820e5e0957e71c75",
            "9e2a0652770d4206a8032031f31dbe19",
            "6b9d39aa316f49339afe54a60121d53b",
            "23e0a6fd096d42c394ef6fcf0af0d4ce",
            "0e2a190713df4400ac22977a193da1aa",
            "eb9a9981be474af4b4e22e15d33395f4",
            "9f383c4928144eb0ba623ca7dc5d198e",
            "31b223519de24b789db9fd03f9370072",
            "f648c8e4ba264358a4c9c18dcab46819",
            "0417b01a056946b89d10e1b32101dc7e",
            "92dc85632f9846b4a9dfe1d47a4e1307",
            "b63ee48a2c034d04a2f1d8ec9a6c3819",
            "370b3b4a656242d1a7aadc39026bb162",
            "ef58dbbe67a541b3a0804596fd968866",
            "5f7844a41dbb4b729ac6df087a02ba56",
            "14f293f3a2c64bf0989e77a99becbd0b",
            "a083d74c00704211ac4edc63eb1fd434"
          ]
        },
        "outputId": "64cfc35c-244e-46ca-e75e-2754e604c3f7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1ae4f511d4b413b938bd84a5d1af7b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a887123175bb4ff587f220243b873edb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f9aaa2486954755b0501a8608673ede"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3f7bc00997341e9b816073d72f018e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f383c4928144eb0ba623ca7dc5d198e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sample IDs (first 20): [1, 24449, 8118, 3358, 369, 3125, 989, 2202, 5585, 354, 272, 907, 727, 297, 559, 1411, 28723, 415, 3358, 2774]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "TOKENIZER_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "sample = cleaned['train'][0][text_field] if 'train' in cleaned else list(cleaned.values())[0][0][text_field]\n",
        "encoded = tokenizer(sample, truncation=True, max_length=128)\n",
        "print(\"Tokenized sample IDs (first 20):\", encoded['input_ids'][:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9383109",
      "metadata": {
        "id": "b9383109"
      },
      "source": [
        "## Step 3. Tokenize dataset and chunk for CLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "40c650b3",
      "metadata": {
        "id": "40c650b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "827e7f577fe04fc8b913ddbe9c411e7a",
            "183c9cf824c44ac7beb4c9c6d1958e20",
            "6a48e458ffeb44509aa06a8375926d3f",
            "d77c4892844041e1aa00a8cf039e7f34",
            "2494502963e64d539f57337b82558300",
            "3080119ad13d40eaa2ee8cd5ce551943",
            "702c801a854b43a698da9ab132feacaf",
            "a438b7f1fc434bc685fb6bb8f51f95ed",
            "10b7b3a53dd44a74b1a24157d051655c",
            "6307208e797d4176a24f56df57e89ce8",
            "2992155792944ebdad21db54c0f25801"
          ]
        },
        "outputId": "19cf5db9-b6ab-479e-9aca-c459e7113c13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|          | 0/180138 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "827e7f577fe04fc8b913ddbe9c411e7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask'],\n",
            "        num_rows: 180138\n",
            "    })\n",
            "})\n",
            "input_ids <class 'list'> <class 'list'>\n",
            "attention_mask <class 'list'> <class 'list'>\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "from itertools import chain\n",
        "SEQ_LEN = 1024\n",
        "\n",
        "def tokenize_function(examples, text_key):\n",
        "    return tokenizer(examples[text_key], truncation=False)\n",
        "\n",
        "# Remove all original columns so only token arrays remain\n",
        "remove_cols = cleaned['train'].column_names\n",
        "tokenized = cleaned.map(\n",
        "    partial(tokenize_function, text_key=text_field),\n",
        "    batched=True,\n",
        "    remove_columns=remove_cols,\n",
        "    desc='Tokenizing',\n",
        ")\n",
        "\n",
        "# Sanity check: ensure tokenized has only token-array columns\n",
        "print(tokenized)\n",
        "batch = tokenized['train'][:2]\n",
        "for k, v in batch.items():\n",
        "    print(k, type(v), type(v[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbf09ecf",
      "metadata": {
        "id": "cbf09ecf"
      },
      "source": [
        "## Step 4. Save tokenized dataset and preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5495d1e9",
      "metadata": {
        "id": "5495d1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "294cf9a0644b4c30a6fea0985bcea325",
            "fd473e87edcb40b2babcf2d270ef741a",
            "af33480e1b284c519edaacaaafc52d96",
            "07c8663e3ef24aff856a0ddcb0e62548",
            "7d2cb43357de42adb82f1a330acf7513",
            "a3276fb6da154162ad910598861f1fe1",
            "81b98cb09a064084a4d07bc7576520b7",
            "cbf44e914a014f76a75e2389982fe846",
            "94e5c886c9c840c08791f7b6023f3edc",
            "eb17d6fcf30c4c98bb2eae58490f7acf",
            "0b265433b6ae4df585c2a061ae9a67fd"
          ]
        },
        "outputId": "e0e37da3-08b1-4a5e-9ab6-e45caab71ccf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Grouping into fixed-length chunks:   0%|          | 0/180138 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "294cf9a0644b4c30a6fea0985bcea325"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 121736\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from itertools import chain\n",
        "\n",
        "def group_texts(examples):\n",
        "    valid_keys = [k for k, v in examples.items() if isinstance(v, list) and v and isinstance(v[0], list)]\n",
        "    concatenated = {k: list(chain.from_iterable(examples[k])) for k in valid_keys}\n",
        "    total_length = len(concatenated['input_ids'])\n",
        "    total_length = (total_length // SEQ_LEN) * SEQ_LEN\n",
        "    result = {}\n",
        "    for k, t in concatenated.items():\n",
        "        result[k] = [t[i:i+SEQ_LEN] for i in range(0, total_length, SEQ_LEN)]\n",
        "    result['labels'] = list(result['input_ids'])\n",
        "    return result\n",
        "\n",
        "lm_datasets = tokenized.map(group_texts, batched=True, desc='Grouping into fixed-length chunks')\n",
        "print(lm_datasets)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {},
        "version_major": 2,
        "version_minor": 0
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}