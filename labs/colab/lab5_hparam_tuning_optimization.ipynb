{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZIBMztuDWSj"
      },
      "source": [
        "# Lab 5 – Hyperparameter Tuning and Optimization\n",
        "Run a small sweep over LR, LoRA rank, and gradient accumulation. Pick best by validation perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyVH2lmsDWSl"
      },
      "source": [
        "## Step 0. Stable installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IcQb72_DWSm"
      },
      "outputs": [],
      "source": [
        "%pip install -q --force-reinstall numpy==2.0.2 pandas==2.2.2 pyarrow==17.0.0\n",
        "%pip install -q datasets>=3.0.0 transformers>=4.41.0 peft>=0.11.0 accelerate>=0.29.0 sentencepiece>=0.1.99 tqdm>=4.66.0 bitsandbytes\n",
        "print('If imports fail, use Runtime → Restart runtime and re-run this cell.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMnGPDn1DWSn"
      },
      "source": [
        "## Step 1. Auto-detect dataset in Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPzegyssDWSn"
      },
      "outputs": [],
      "source": [
        "# Step 1: Auto-detect a valid saved dataset in Drive (and verify shards)\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import os, json\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE = Path('/content/drive/MyDrive/slm-labs')\n",
        "assert BASE.exists(), f\"Base folder not found: {BASE}. Create it or change BASE.\"\n",
        "\n",
        "def is_valid_hf_dataset(root: Path) -> bool:\n",
        "    \"\"\"A quick sanity check for a saved load_from_disk dataset.\"\"\"\n",
        "    if not (root / \"dataset_info.json\").exists():\n",
        "        return False\n",
        "    # Must have at least a 'train' directory with arrow shards and index\n",
        "    train_dir = root / \"train\"\n",
        "    if not train_dir.exists():\n",
        "        return False\n",
        "    # At least one .arrow shard and an index/ state json is expected\n",
        "    has_arrow = any(p.suffix == \".arrow\" for p in train_dir.glob(\"*.arrow\"))\n",
        "    has_index = any(p.suffix == \".json\" for p in root.glob(\"*.json\"))  # dataset_state.json etc.\n",
        "    return has_arrow and has_index\n",
        "\n",
        "candidates = []\n",
        "for r, ds, fs in os.walk(BASE):\n",
        "    p = Path(r)\n",
        "    if (p / \"dataset_info.json\").exists():\n",
        "        if is_valid_hf_dataset(p):\n",
        "            candidates.append(p)\n",
        "\n",
        "print(\"Valid saved datasets found:\")\n",
        "for i, p in enumerate(candidates, 1):\n",
        "    print(f\"{i}. {p}\")\n",
        "\n",
        "# Pick the first valid one by default, but you can set DATA_DIR manually after it prints\n",
        "DATA_DIR = candidates[0] if candidates else None\n",
        "print(\"Using DATA_DIR:\", DATA_DIR)\n",
        "\n",
        "assert DATA_DIR is not None, (\n",
        "    \"No valid dataset folder found. Re-run Lab 3 and save with \"\n",
        "    \"ds.save_to_disk('/content/drive/MyDrive/slm-labs/lab3_tokenized')\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV5kBDLPDWSo"
      },
      "source": [
        "## Step 2. Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnYEso_MDWSo"
      },
      "outputs": [],
      "source": [
        "# Step 2: Load dataset (works only if Step 1 verified a proper folder)\n",
        "from datasets import load_from_disk\n",
        "ds = load_from_disk(str(DATA_DIR))\n",
        "print(ds)\n",
        "val = ds.get(\"validation\") or ds.get(\"test\")\n",
        "if val is None:\n",
        "    # fall back to a small slice of train for quick checks\n",
        "    val = ds[\"train\"].select(range(min(200, len(ds[\"train\"]))))\n",
        "print(\"Validation samples:\", len(val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f0UCipMDWSo"
      },
      "source": [
        "## Step 3. Load base model (4-bit if possible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtwwL4RODWSp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "BASE_MODEL='HuggingFaceH4/zephyr-7b-beta'\n",
        "kw={}\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        kw=dict(device_map='auto', quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True), torch_dtype=torch.float16)\n",
        "    except Exception:\n",
        "        kw=dict(torch_dtype=torch.float16)\n",
        "else:\n",
        "    kw=dict(torch_dtype=torch.float32)\n",
        "Tok=AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "Model=AutoModelForCausalLM.from_pretrained(BASE_MODEL, **kw)\n",
        "if Tok.pad_token is None: Tok.pad_token=Tok.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv_HVAdKDWSp"
      },
      "source": [
        "## Step 4. Attach LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uaX1UeYDWSp"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "TARGETS=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "def attach_lora(m, r=16, alpha=32, drop=0.05):\n",
        "    m = prepare_model_for_kbit_training(m)\n",
        "    cfg = LoraConfig(r=r, lora_alpha=alpha, lora_dropout=drop, target_modules=TARGETS, bias='none', task_type='CAUSAL_LM')\n",
        "    pm = get_peft_model(m, cfg)\n",
        "    pm.print_trainable_parameters()\n",
        "    return pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cgjOSEcDWSp"
      },
      "source": [
        "## Step 5. Train short and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3rNQ7e9DWSq"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import math, time, pandas as pd\n",
        "\n",
        "def run(h):\n",
        "    m = attach_lora(Model, r=h['r'], alpha=h['alpha'], drop=h['drop'])\n",
        "    coll = DataCollatorForLanguageModeling(tokenizer=Tok, mlm=False)\n",
        "    args = TrainingArguments(output_dir=f\"./out_{int(time.time())}\", per_device_train_batch_size=h['bs'], gradient_accumulation_steps=h['ga'], learning_rate=h['lr'], warmup_steps=10, max_steps=h['steps'], logging_steps=10, save_strategy='no', fp16=torch.cuda.is_available(), report_to=[])\n",
        "    trainer = Trainer(model=m, args=args, train_dataset=ds['train'], eval_dataset=val, data_collator=coll)\n",
        "    trainer.train()\n",
        "    ev = trainer.evaluate() if val else {}\n",
        "    loss = ev.get('eval_loss', None)\n",
        "    ppl = math.exp(loss) if loss else None\n",
        "    return {'loss': loss, 'ppl': ppl}, m\n",
        "\n",
        "search=[{'lr':2e-4,'r':16,'alpha':32,'drop':0.05,'bs':2,'ga':4,'steps':100}]\n",
        "recs=[]; BEST=None; BEST_MODEL=None\n",
        "for h in search:\n",
        "    print('Trial',h)\n",
        "    mtr, m=run(h)\n",
        "    row={**h, **mtr}; recs.append(row)\n",
        "    if BEST is None or (mtr['ppl'] and mtr['ppl']<BEST['ppl']):\n",
        "        BEST=row; BEST_MODEL=m\n",
        "DF=pd.DataFrame(recs)\n",
        "display(DF)\n",
        "print('Best:',BEST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnTctDxADWSq"
      },
      "source": [
        "## Step 6. Save results to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4EF5reRDWSq"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "RES = Path('/content/drive/MyDrive/slm-labs/lab5_results'); RES.mkdir(parents=True, exist_ok=True)\n",
        "DF.to_csv(RES/'trials.csv', index=False)\n",
        "if BEST_MODEL is not None:\n",
        "    tag=f\"r{BEST['r']}_lr{BEST['lr']}_ga{BEST['ga']}\"; sd=RES/f\"best_{tag}\"; sd.mkdir(parents=True, exist_ok=True)\n",
        "    BEST_MODEL.save_pretrained(sd); Tok.save_pretrained(sd)\n",
        "    print('Saved best adapters to', sd)\n",
        "else:\n",
        "    print('No best model to save')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}