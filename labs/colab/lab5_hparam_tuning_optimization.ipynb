{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZIBMztuDWSj"
      },
      "source": [
        "# Lab 5 – Hyperparameter Tuning and Optimization\n",
        "Run a small sweep over LR, LoRA rank, and gradient accumulation. Pick best by validation perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyVH2lmsDWSl"
      },
      "source": [
        "## Step 0. Stable installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3IcQb72_DWSm",
        "outputId": "361fff3e-8e57-4077-adc4-7733f97c5259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIf imports fail, use Runtime → Restart runtime and re-run this cell.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q --force-reinstall numpy==2.0.2 pandas==2.2.2 pyarrow==17.0.0\n",
        "%pip install -q datasets>=3.0.0 transformers>=4.41.0 peft>=0.11.0 accelerate>=0.29.0 sentencepiece>=0.1.99 tqdm>=4.66.0 bitsandbytes\n",
        "print('If imports fail, use Runtime → Restart runtime and re-run this cell.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMnGPDn1DWSn"
      },
      "source": [
        "## Step 1. Auto-detect dataset in Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPzegyssDWSn"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load dataset saved from Lab 3\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "from datasets import load_from_disk\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Consistent base directory for all labs\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/slm-labs\")\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Dataset directory from Lab 3\n",
        "DATA_DIR = BASE_DIR / \"lab3_tokenized\"\n",
        "\n",
        "# Sanity checks\n",
        "assert DATA_DIR.exists(), f\"Dataset not found at {DATA_DIR}. Run Lab 3 first.\"\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_from_disk(DATA_DIR)\n",
        "print(\"Loaded dataset from:\", DATA_DIR)\n",
        "print(\"Splits:\", list(dataset.keys()))\n",
        "for split, dset in dataset.items():\n",
        "    print(f\"{split}: {len(dset)} rows, columns = {dset.column_names}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV5kBDLPDWSo"
      },
      "source": [
        "## Step 2. Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnYEso_MDWSo"
      },
      "outputs": [],
      "source": [
        "# Step 2: Load dataset (works only if Step 1 verified a proper folder)\n",
        "from datasets import load_from_disk\n",
        "ds = load_from_disk(str(DATA_DIR))\n",
        "print(ds)\n",
        "val = ds.get(\"validation\") or ds.get(\"test\")\n",
        "if val is None:\n",
        "    # fall back to a small slice of train for quick checks\n",
        "    val = ds[\"train\"].select(range(min(200, len(ds[\"train\"]))))\n",
        "print(\"Validation samples:\", len(val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f0UCipMDWSo"
      },
      "source": [
        "## Step 3. Load base model (4-bit if possible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtwwL4RODWSp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "BASE_MODEL='HuggingFaceH4/zephyr-7b-beta'\n",
        "kw={}\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        kw=dict(device_map='auto', quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True), torch_dtype=torch.float16)\n",
        "    except Exception:\n",
        "        kw=dict(torch_dtype=torch.float16)\n",
        "else:\n",
        "    kw=dict(torch_dtype=torch.float32)\n",
        "Tok=AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "Model=AutoModelForCausalLM.from_pretrained(BASE_MODEL, **kw)\n",
        "if Tok.pad_token is None: Tok.pad_token=Tok.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv_HVAdKDWSp"
      },
      "source": [
        "## Step 4. Attach LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uaX1UeYDWSp"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "TARGETS=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "def attach_lora(m, r=16, alpha=32, drop=0.05):\n",
        "    m = prepare_model_for_kbit_training(m)\n",
        "    cfg = LoraConfig(r=r, lora_alpha=alpha, lora_dropout=drop, target_modules=TARGETS, bias='none', task_type='CAUSAL_LM')\n",
        "    pm = get_peft_model(m, cfg)\n",
        "    pm.print_trainable_parameters()\n",
        "    return pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cgjOSEcDWSp"
      },
      "source": [
        "## Step 5. Train short and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3rNQ7e9DWSq"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import math, time, pandas as pd\n",
        "\n",
        "def run(h):\n",
        "    m = attach_lora(Model, r=h['r'], alpha=h['alpha'], drop=h['drop'])\n",
        "    coll = DataCollatorForLanguageModeling(tokenizer=Tok, mlm=False)\n",
        "    args = TrainingArguments(output_dir=f\"./out_{int(time.time())}\", per_device_train_batch_size=h['bs'], gradient_accumulation_steps=h['ga'], learning_rate=h['lr'], warmup_steps=10, max_steps=h['steps'], logging_steps=10, save_strategy='no', fp16=torch.cuda.is_available(), report_to=[])\n",
        "    trainer = Trainer(model=m, args=args, train_dataset=ds['train'], eval_dataset=val, data_collator=coll)\n",
        "    trainer.train()\n",
        "    ev = trainer.evaluate() if val else {}\n",
        "    loss = ev.get('eval_loss', None)\n",
        "    ppl = math.exp(loss) if loss else None\n",
        "    return {'loss': loss, 'ppl': ppl}, m\n",
        "\n",
        "search=[{'lr':2e-4,'r':16,'alpha':32,'drop':0.05,'bs':2,'ga':4,'steps':100}]\n",
        "recs=[]; BEST=None; BEST_MODEL=None\n",
        "for h in search:\n",
        "    print('Trial',h)\n",
        "    mtr, m=run(h)\n",
        "    row={**h, **mtr}; recs.append(row)\n",
        "    if BEST is None or (mtr['ppl'] and mtr['ppl']<BEST['ppl']):\n",
        "        BEST=row; BEST_MODEL=m\n",
        "DF=pd.DataFrame(recs)\n",
        "display(DF)\n",
        "print('Best:',BEST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnTctDxADWSq"
      },
      "source": [
        "## Step 6. Save results to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4EF5reRDWSq"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "RES = Path('/content/drive/MyDrive/slm-labs/lab5_results'); RES.mkdir(parents=True, exist_ok=True)\n",
        "DF.to_csv(RES/'trials.csv', index=False)\n",
        "if BEST_MODEL is not None:\n",
        "    tag=f\"r{BEST['r']}_lr{BEST['lr']}_ga{BEST['ga']}\"; sd=RES/f\"best_{tag}\"; sd.mkdir(parents=True, exist_ok=True)\n",
        "    BEST_MODEL.save_pretrained(sd); Tok.save_pretrained(sd)\n",
        "    print('Saved best adapters to', sd)\n",
        "else:\n",
        "    print('No best model to save')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}