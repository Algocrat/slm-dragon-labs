{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed6ac39",
   "metadata": {},
   "source": [
    "# Lab 2 – Base Model Loading and Inference\n",
    "**Part 2 of the 7 Lab Hands-On SLM Training Series**\n",
    "\n",
    "This lab builds on the concepts from the post **How to Train Your Dragon: Customize a Small Language Model for Your Domain**:\n",
    "https://www.linkedin.com/pulse/how-train-your-dragon-customize-small-language-model-domain-patel-cvzjc\n",
    "\n",
    "In Lab 1, we established a clean, reproducible environment. In Lab 2, we will:\n",
    "- Load a high-quality Small Language Model (SLM) in 4-bit to fit common GPUs.\n",
    "- Run initial prompts to understand the base model’s behavior.\n",
    "- Explore generation parameters (temperature, top_p, max_new_tokens, repetition_penalty).\n",
    "- Log results for later comparison after fine-tuning.\n",
    "\n",
    "> Tip: If you hit out-of-memory on the default model, switch to the TinyLlama fallback included below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b4b8c6",
   "metadata": {},
   "source": [
    "## 1. Runtime and GPU Check\n",
    "Run the cell below to confirm your runtime has a GPU. On Google Colab, choose **Runtime → Change runtime type → T4 or L4 GPU**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check GPU\n",
    "import os, sys, subprocess, textwrap\n",
    "\n",
    "def _run(cmd):\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True, text=True, timeout=30)\n",
    "        print(out)\n",
    "    except Exception as e:\n",
    "        print(f\"Command failed: {cmd}\\n{e}\")\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"CUDA visible devices:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "\n",
    "print(\"\\n=== nvidia-smi ===\")\n",
    "_run(\"nvidia-smi || echo 'No NVIDIA GPU found'\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"\\nPyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "        print(\"Total VRAM (GB):\", round(torch.cuda.get_device_properties(0).total_memory/1e9, 2))\n",
    "except Exception as e:\n",
    "    print(\"Torch not installed yet. Will be installed next.\\n\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9951749",
   "metadata": {},
   "source": [
    "## 2. Install Required Libraries\n",
    "We use:\n",
    "- `transformers` for model and tokenizer\n",
    "- `accelerate` for device placement and mixed precision\n",
    "- `bitsandbytes` for 4-bit quantization\n",
    "- `sentencepiece` for tokenizers used by Mistral/LLaMA\n",
    "\n",
    "Rerun this cell if installs time out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install libraries\n",
    "!pip -q install --upgrade transformers accelerate bitsandbytes sentencepiece > /dev/null\n",
    "\n",
    "import importlib, sys\n",
    "mods = [\"transformers\", \"accelerate\", \"bitsandbytes\", \"sentencepiece\"]\n",
    "for m in mods:\n",
    "    try:\n",
    "        importlib.import_module(m)\n",
    "        print(f\"OK: {m}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Missing: {m}\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d07f6",
   "metadata": {},
   "source": [
    "## 3. Choose a Base Model\n",
    "Default is **mistralai/Mistral-7B-Instruct-v0.2** (strong, efficient).  \n",
    "If you hit memory errors, switch to the fallback **TinyLlama/TinyLlama-1.1B-Chat-v1.0**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03631007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select your base model\n",
    "#@markdown Recommended default: mistralai/Mistral-7B-Instruct-v0.2\n",
    "#@markdown Fallback if you see CUDA OOM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"  #@param [\"mistralai/Mistral-7B-Instruct-v0.2\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"]\n",
    "print(\"Using model:\", model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a7e7c",
   "metadata": {},
   "source": [
    "## 4. Load the Model in 4-bit (bnb.int4)\n",
    "This reduces VRAM requirements and makes large models usable on consumer GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load model and tokenizer (4-bit)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch, math\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(\"Loaded:\", model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b253f3",
   "metadata": {},
   "source": [
    "## 5. Inference Helper\n",
    "The function below formats prompts, applies generation parameters, and returns decoded text.  \n",
    "It auto-detects chat templates when available (e.g., Mistral Instruct).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c516fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define a generate_text helper\n",
    "from typing import Optional, Dict\n",
    "\n",
    "def apply_chat_template(tokenizer, user_prompt: str, system_prompt: Optional[str] = None):\n",
    "    # Use chat template if provided by the tokenizer (common for Instruct models)\n",
    "    if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    # Fallback: plain prompt\n",
    "    if system_prompt:\n",
    "        return f\"System: {system_prompt}\\nUser: {user_prompt}\\nAssistant:\"\n",
    "    return user_prompt\n",
    "\n",
    "def generate_text(\n",
    "    prompt: str,\n",
    "    system: Optional[str] = None,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    do_sample: bool = True,\n",
    ") -> str:\n",
    "    text = apply_chat_template(tokenizer, prompt, system_prompt=system)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    # Remove the input portion if needed\n",
    "    gen_ids = output_ids[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133da8d",
   "metadata": {},
   "source": [
    "## 6. Quick Smoke Test\n",
    "Run a couple of prompts to observe out-of-the-box behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Try a few prompts\n",
    "system_prompt = \"You are a concise, accurate assistant. Answer clearly and avoid speculation.\"\n",
    "prompts = [\n",
    "    \"Summarize the core differences between SLMs and LLMs for an executive audience.\",\n",
    "    \"Give three use cases where a domain-tuned SLM can outperform a general-purpose LLM.\",\n",
    "    \"Rewrite this sentence in a more formal tone: 'We gotta cut costs but keep quality.'\",\n",
    "]\n",
    "\n",
    "for i, p in enumerate(prompts, 1):\n",
    "    print(f\"--- Prompt {i} ---\")\n",
    "    out = generate_text(p, system=system_prompt, max_new_tokens=200, temperature=0.7, top_p=0.9)\n",
    "    print(out.strip(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f535effd",
   "metadata": {},
   "source": [
    "## 7. Parameter Exploration\n",
    "Experiment with generation settings to see their effect on style and determinism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Parameter sweep\n",
    "base_prompt = \"Explain retrieval-augmented generation in two short paragraphs.\"\n",
    "sweep = [\n",
    "    dict(temperature=0.2, top_p=0.9, repetition_penalty=1.05),\n",
    "    dict(temperature=0.7, top_p=0.9, repetition_penalty=1.05),\n",
    "    dict(temperature=1.0, top_p=0.95, repetition_penalty=1.05),\n",
    "]\n",
    "\n",
    "for cfg in sweep:\n",
    "    print(f\"\\n### Settings: {cfg}\")\n",
    "    txt = generate_text(\n",
    "        base_prompt,\n",
    "        system=\"You are an expert technical writer.\",\n",
    "        max_new_tokens=220,\n",
    "        temperature=cfg[\"temperature\"],\n",
    "        top_p=cfg[\"top_p\"],\n",
    "        repetition_penalty=cfg[\"repetition_penalty\"],\n",
    "    )\n",
    "    print(txt.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcadfbbe",
   "metadata": {},
   "source": [
    "## 8. Structured Logging\n",
    "Use this template to log prompts, parameters, and outputs for later comparison (e.g., in Lab 6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Log results to CSV\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "log_path = Path(\"/content\") if Path(\"/content\").exists() else Path(\"/mnt/data\")\n",
    "csv_file = log_path / \"lab2_inference_log.csv\"\n",
    "\n",
    "records = []\n",
    "records.append({\n",
    "    \"timestamp\": pd.Timestamp.utcnow().isoformat(),\n",
    "    \"model_name\": model_name,\n",
    "    \"prompt\": \"Explain retrieval-augmented generation in two short paragraphs.\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_new_tokens\": 220,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"output\": generate_text(\"Explain retrieval-augmented generation in two short paragraphs.\", system=\"You are an expert technical writer.\", max_new_tokens=220),\n",
    "})\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "if csv_file.exists():\n",
    "    df_existing = pd.read_csv(csv_file)\n",
    "    df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(\"Logged results to:\", csv_file)\n",
    "df.tail(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c56b97",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "- Capture a small suite of representative prompts from your domain and log baseline outputs.\n",
    "- Identify weaknesses you expect fine-tuning to address (terminology use, formatting, factual grounding).\n",
    "- Save your `lab2_inference_log.csv` for use in Lab 6 (evaluation and comparison).\n",
    "\n",
    "When ready, proceed to **Lab 3 – Data Loading and Tokenization**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
