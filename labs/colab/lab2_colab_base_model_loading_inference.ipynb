{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91fa9c7",
   "metadata": {},
   "source": [
    "# Lab 2 – Base Model Loading and Inference\n",
    "**Part 2 of the 7 Lab Hands-On SLM Training Series**\n",
    "\n",
    "This lab builds on the concepts from the post **How to Train Your Dragon: Customize a Small Language Model for Your Domain**:\n",
    "https://www.linkedin.com/pulse/how-train-your-dragon-customize-small-language-model-domain-patel-cvzjc\n",
    "\n",
    "In Lab 1, we established a clean, reproducible environment. In Lab 2, we will:\n",
    "- Load a high-quality Small Language Model (SLM) in 4-bit to fit common GPUs.\n",
    "- Run initial prompts to understand the base model’s behavior.\n",
    "- Explore generation parameters (temperature, top_p, max_new_tokens, repetition_penalty).\n",
    "- Log results for later comparison after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d44492",
   "metadata": {},
   "source": [
    "## 1. Runtime and GPU Check\n",
    "On Google Colab, choose **Runtime → Change runtime type → GPU** (T4 or L4 is fine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and PyTorch CUDA\n",
    "import os, sys, subprocess, textwrap\n",
    "\n",
    "def _run(cmd):\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True, text=True, timeout=30)\n",
    "        print(out)\n",
    "    except Exception as e:\n",
    "        print(f\"Command failed: {cmd}\\n{e}\")\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"CUDA visible devices:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "\n",
    "print(\"\\n=== nvidia-smi ===\")\n",
    "_run(\"nvidia-smi || echo 'No NVIDIA GPU found'\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"\\nPyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "        print(\"Total VRAM (GB):\", round(torch.cuda.get_device_properties(0).total_memory/1e9, 2))\n",
    "except Exception as e:\n",
    "    print(\"Torch not installed yet. Will be installed next.\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb50d9",
   "metadata": {},
   "source": [
    "## 2. Install Required Libraries\n",
    "We use:\n",
    "- `transformers` for model and tokenizer\n",
    "- `accelerate` for device placement and mixed precision\n",
    "- `bitsandbytes` for 4-bit quantization\n",
    "- `sentencepiece` for tokenizers used by Mistral/LLaMA\n",
    "- `huggingface_hub` for authentication to gated models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc5d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip -q install --upgrade transformers accelerate bitsandbytes sentencepiece huggingface_hub > /dev/null\n",
    "\n",
    "import importlib\n",
    "for m in [\"transformers\", \"accelerate\", \"bitsandbytes\", \"sentencepiece\", \"huggingface_hub\"]:\n",
    "    try:\n",
    "        importlib.import_module(m)\n",
    "        print(f\"OK: {m}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Missing: {m}\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ca206",
   "metadata": {},
   "source": [
    "## 3. Choose a Base Model\n",
    "Recommended non-gated default: **HuggingFaceH4/zephyr-7b-beta**  \n",
    "If you have access to a gated model (e.g., Mistral or LLaMA), you can select it after authenticating in 3b.  \n",
    "TinyLlama is provided as a lightweight fallback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select your base model\n",
    "#@markdown Recommended default: HuggingFaceH4/zephyr-7b-beta (public)\n",
    "#@markdown If you have access tokens and have accepted licenses, you can try gated models.\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"  #@param [\"HuggingFaceH4/zephyr-7b-beta\", \"mistralai/Mistral-7B-Instruct-v0.2\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"meta-llama/Llama-3.1-8B-Instruct\"]\n",
    "print(\"Using model:\", model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f100e1c2",
   "metadata": {},
   "source": [
    "## 3b. (Optional) Authenticate to Hugging Face\n",
    "Some models are gated. To use them, you must:\n",
    "1. Visit the model page and click **Access repository** (accept license/terms).\n",
    "2. Create a token at https://huggingface.co/settings/tokens (read access is enough).\n",
    "3. Run the cell below and paste your token when prompted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login prompt (safe to skip if using public models)\n",
    "from huggingface_hub import login\n",
    "try:\n",
    "    login()  # This will open an interactive prompt in Colab\n",
    "except Exception as e:\n",
    "    print(\"Login skipped or not needed.\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58cee2f",
   "metadata": {},
   "source": [
    "## 4. Load the Model in 4-bit (bnb.int4)\n",
    "This reduces VRAM requirements and makes larger models usable on consumer GPUs.\n",
    "If the selected model is gated or unavailable, we will automatically fall back to a public model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428464f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "PREFERRED_FALLBACKS = [\"HuggingFaceH4/zephyr-7b-beta\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"]\n",
    "\n",
    "def try_load(name: str):\n",
    "    print(f\"Attempting to load: {name}\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    tok = AutoTokenizer.from_pretrained(name, use_fast=True, trust_remote_code=True)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok, mdl\n",
    "\n",
    "tokenizer = model = None\n",
    "tried = []\n",
    "candidates = [model_name] + [m for m in PREFERRED_FALLBACKS if m != model_name]\n",
    "last_err = None\n",
    "\n",
    "for cand in candidates:\n",
    "    try:\n",
    "        tokenizer, model = try_load(cand)\n",
    "        model_name = cand\n",
    "        print(f\"Loaded: {cand}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "        tried.append(cand)\n",
    "        print(f\"Failed to load {cand}: {e}\\n\")\n",
    "\n",
    "if model is None:\n",
    "    raise RuntimeError(f\"Could not load any model. Tried {tried}. Last error: {last_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b9e4f",
   "metadata": {},
   "source": [
    "## 5. Inference Helper\n",
    "The function below formats prompts, applies generation parameters, and returns decoded text.\n",
    "It auto-detects chat templates when available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def apply_chat_template(tokenizer, user_prompt: str, system_prompt: Optional[str] = None):\n",
    "    if hasattr(tokenizer, \"apply_chat_template\") and getattr(tokenizer, \"chat_template\", None):\n",
    "        msgs = []\n",
    "        if system_prompt:\n",
    "            msgs.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        msgs.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "        return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "    if system_prompt:\n",
    "        return f\"System: {system_prompt}\\nUser: {user_prompt}\\nAssistant:\"\n",
    "    return user_prompt\n",
    "\n",
    "def generate_text(\n",
    "    prompt: str,\n",
    "    system: Optional[str] = None,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    do_sample: bool = True,\n",
    ") -> str:\n",
    "    text = apply_chat_template(tokenizer, prompt, system_prompt=system)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    gen_ids = output_ids[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56603ec",
   "metadata": {},
   "source": [
    "## 6. Quick Smoke Test\n",
    "Run a couple of prompts to observe out-of-the-box behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a concise, accurate assistant. Answer clearly and avoid speculation.\"\n",
    "prompts = [\n",
    "    \"Summarize the core differences between SLMs and LLMs for an executive audience.\",\n",
    "    \"Give three use cases where a domain-tuned SLM can outperform a general-purpose LLM.\",\n",
    "    \"Rewrite this sentence in a more formal tone: 'We gotta cut costs but keep quality.'\",\n",
    "]\n",
    "\n",
    "for i, p in enumerate(prompts, 1):\n",
    "    print(f\"--- Prompt {i} ---\")\n",
    "    out = generate_text(p, system=system_prompt, max_new_tokens=200, temperature=0.7, top_p=0.9)\n",
    "    print(out.strip(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaffe043",
   "metadata": {},
   "source": [
    "## 7. Parameter Exploration\n",
    "Experiment with generation settings to see their effect on style and determinism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"Explain retrieval-augmented generation in two short paragraphs.\"\n",
    "sweep = [\n",
    "    dict(temperature=0.2, top_p=0.9, repetition_penalty=1.05),\n",
    "    dict(temperature=0.7, top_p=0.9, repetition_penalty=1.05),\n",
    "    dict(temperature=1.0, top_p=0.95, repetition_penalty=1.05),\n",
    "]\n",
    "\n",
    "for cfg in sweep:\n",
    "    print(f\"\\n### Settings: {cfg}\")\n",
    "    txt = generate_text(\n",
    "        base_prompt,\n",
    "        system=\"You are an expert technical writer.\",\n",
    "        max_new_tokens=220,\n",
    "        temperature=cfg[\"temperature\"],\n",
    "        top_p=cfg[\"top_p\"],\n",
    "        repetition_penalty=cfg[\"repetition_penalty\"],\n",
    "    )\n",
    "    print(txt.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb737c",
   "metadata": {},
   "source": [
    "## 8. Structured Logging\n",
    "Use this template to log prompts, parameters, and outputs for later comparison (e.g., in Lab 6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c05d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "log_path = Path(\"/content\") if Path(\"/content\").exists() else Path(\"/mnt/data\")\n",
    "csv_file = log_path / \"lab2_inference_log.csv\"\n",
    "\n",
    "records = []\n",
    "records.append({\n",
    "    \"model_name\": model_name,\n",
    "    \"prompt\": \"Explain retrieval-augmented generation in two short paragraphs.\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_new_tokens\": 220,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"output\": generate_text(\"Explain retrieval-augmented generation in two short paragraphs.\", system=\"You are an expert technical writer.\", max_new_tokens=220),\n",
    "})\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "if csv_file.exists():\n",
    "    df_existing = pd.read_csv(csv_file)\n",
    "    df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(\"Logged results to:\", csv_file)\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d0a23",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "- Capture a small suite of representative prompts from your domain and log baseline outputs.\n",
    "- Identify weaknesses you expect fine-tuning to address (terminology use, formatting, factual grounding).\n",
    "- Save your `lab2_inference_log.csv` for use in Lab 6 (evaluation and comparison).\n",
    "\n",
    "When ready, proceed to **Lab 3 – Data Loading and Tokenization**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
