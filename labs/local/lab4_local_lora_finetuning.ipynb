{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce03e2e",
   "metadata": {},
   "source": [
    "# Lab 4 – LoRA Fine-Tuning\n",
    "**Part 4 of the 7 Lab Hands-On SLM Training Series**\n",
    "\n",
    "In this lab, we fine-tune a Small Language Model (SLM) on your domain data using **LoRA** (Low‑Rank Adaptation) with the `peft` library. This notebook is designed to be stable in Google local machine and to load the processed dataset saved in Lab 3 from your local filesystem.\n",
    "\n",
    "**Outcome**\n",
    "• Attach LoRA adapters to a base model\n",
    "• Run a short fine‑tuning loop on your tokenized dataset\n",
    "• Save the LoRA adapters back to Drive for reuse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158862f",
   "metadata": {},
   "source": [
    "## Step 0. Stable installs for local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda6cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q --force-reinstall \"numpy==2.0.2\" \"pandas==2.2.2\" \"pyarrow==17.0.0\"\n",
    "pip install -q \"datasets>=3.0.0\" \"transformers>=4.41.0\" \"peft>=0.11.0\" \"accelerate>=0.29.0\" \"sentencepiece>=0.1.99\" \"tqdm>=4.66.0\" bitsandbytes\n",
    "\n",
    "import importlib, traceback\n",
    "mods = [\"numpy\", \"pandas\", \"pyarrow\", \"datasets\", \"transformers\", \"peft\", \"accelerate\", \"sentencepiece\", \"tqdm\"]\n",
    "for m in mods:\n",
    "    try:\n",
    "        mod = importlib.import_module(m)\n",
    "        print(f\"{m}: {getattr(mod, '__version__', 'unknown')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Import error] {m}: {e}\")\n",
    "print(\"If any import failed, go to Runtime → Restart runtime, then re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038967a2",
   "metadata": {},
   "source": [
    "## Step 1. Load the prepared dataset from your local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "# from google.colab import drive  # not needed locally\n",
    "# drive.mount('./drive')  # not needed locally\n",
    "\n",
    "DATA_DIR = \"./lab3_tokenized\"  # Path where Lab 3 saved the tokenized dataset\n",
    "dataset = load_from_disk(DATA_DIR)\n",
    "print(dataset)\n",
    "print(\"Train rows:\", len(dataset[\"train\"]))\n",
    "print(\"Columns:\", dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497293b",
   "metadata": {},
   "source": [
    "## Step 2. Load a base model (4‑bit on GPU if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Choose a public model that works well with chat templates and LoRA\n",
    "PREFERRED_MODELS = [\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "]\n",
    "\n",
    "def load_base_model(name: str):\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    print(f\"CUDA available: {use_gpu}\")\n",
    "    quant_cfg = None\n",
    "    kwargs = {}\n",
    "    if use_gpu:\n",
    "        try:\n",
    "            quant_cfg = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            kwargs.update(dict(device_map=\"auto\", quantization_config=quant_cfg, torch_dtype=torch.float16))\n",
    "        except Exception as e:\n",
    "            print(\"bitsandbytes not available, falling back to non-quantized load.\")\n",
    "            kwargs.update(dict(torch_dtype=torch.float16 if use_gpu else torch.float32))\n",
    "    else:\n",
    "        kwargs.update(dict(torch_dtype=torch.float32))\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(name, use_fast=True)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(name, **kwargs)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok, mdl\n",
    "\n",
    "tokenizer = model = None\n",
    "last_err = None\n",
    "for cand in PREFERRED_MODELS:\n",
    "    try:\n",
    "        print(f\"Attempting model: {cand}\")\n",
    "        tokenizer, model = load_base_model(cand)\n",
    "        model_name = cand\n",
    "        print(f\"Loaded: {cand}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "        print(f\"Failed to load {cand}: {e}\")\n",
    "\n",
    "if model is None:\n",
    "    raise RuntimeError(f\"Could not load any model. Last error: {last_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40130bf1",
   "metadata": {},
   "source": [
    "## Step 3. Attach LoRA adapters with PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for k-bit training if quantized\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Common LoRA target modules for decoder-only models (LLaMA/Mistral/Zephyr families)\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63120aad",
   "metadata": {},
   "source": [
    "## Step 4. Fine‑tune with transformers.Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc2ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import math\n",
    "\n",
    "# Use validation split if present; otherwise train only\n",
    "train_ds = dataset[\"train\"]\n",
    "eval_ds = dataset.get(\"validation\") or dataset.get(\"test\")\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.0,\n",
    "    warmup_steps=10,\n",
    "    max_steps=100,  # keep small for demo; increase for real training\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    bf16=False,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(train_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b022a",
   "metadata": {},
   "source": [
    "## Step 5. Save LoRA adapters to your local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "SAVE_DIR = \"./lab4_lora_adapters\"\n",
    "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(\"Saved LoRA adapters to:\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a6103",
   "metadata": {},
   "source": [
    "## Optional: Quick generation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = \"Summarize the key considerations when drafting a cardiology discharge note.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        streamer=streamer,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51494161",
   "metadata": {},
   "source": [
    "### Wrap‑up\n",
    "You have:\n",
    "• Loaded your tokenized dataset from your local filesystem\n",
    "• Attached LoRA adapters with PEFT\n",
    "• Run a short fine‑tuning loop using `transformers.Trainer`\n",
    "• Saved adapters back to Drive for reuse in inference or future training\n",
    "\n",
    "Next up: **Lab 5 – Hyperparameter Tuning and Optimization**."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
