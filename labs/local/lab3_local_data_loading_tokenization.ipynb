{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d4a2bca",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Algocrat/slm-dragon-labs/blob/main/labs/colab/lab3_fresh_data_loading_tokenization_revised_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In local machine\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4481716",
   "metadata": {},
   "source": [
    "# Lab 3 – Data Loading and Tokenization\n",
    "**Part 3 of the 7 Lab Hands-On SLM Training Series**\n",
    "\n",
    "This notebook downloads the `ncbi/Open-Patients` dataset, performs basic cleaning and sanity checks, detects the text field automatically, and prepares tokenized chunks for causal language modeling (CLM). It saves a tokenized dataset to disk for use in Lab 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b265f4",
   "metadata": {},
   "source": [
    "### Note on dataset access and licensing\n",
    "The `ncbi/Open-Patients` dataset is publicly available on the Hugging Face Hub under CC-BY-SA 4.0. No authentication is required to download. Please provide attribution if you reuse the data, and ensure your use complies with the license and any applicable privacy rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566626da",
   "metadata": {},
   "source": [
    "## Step 0. Install dependencies (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable, Colab-friendly deps for SLM labs\n",
    "# - Pins pandas + pyarrow to avoid ABI conflicts with preinstalled GPU libs (RAPIDS)\n",
    "# - Installs HF stack\n",
    "# - Verifies imports and prints versions\n",
    "\n",
    "pip install -q --force-reinstall \"pandas==2.2.2\" \"pyarrow==17.0.0\"\n",
    "pip install -q \"datasets>=2.19.0\" \"transformers>=4.41.0\" \"sentencepiece>=0.1.99\" \"tqdm>=4.66.0\"\n",
    "\n",
    "import importlib, traceback\n",
    "\n",
    "mods = [\"pandas\", \"pyarrow\", \"datasets\", \"transformers\", \"sentencepiece\", \"tqdm\"]\n",
    "ok = True\n",
    "for m in mods:\n",
    "    try:\n",
    "        mod = importlib.import_module(m)\n",
    "        print(f\"{m}: {getattr(mod, '__version__', 'unknown')}\")\n",
    "    except Exception as e:\n",
    "        ok = False\n",
    "        print(f\"[Import error] {m}: {e}\")\n",
    "        traceback.print_exc(limit=1)\n",
    "\n",
    "if ok:\n",
    "    print(\"Dependencies OK\")\n",
    "else:\n",
    "    print(\"\\nOne or more imports failed (usually due to mixed wheels after an upgrade).\")\n",
    "    print(\"Please go to Runtime → Restart runtime, then re-run this cell once.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d09f8c",
   "metadata": {},
   "source": [
    "## Step 1. Download dataset: `ncbi/Open-Patients`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Public dataset under CC-BY-SA 4.0; no authentication required\n",
    "dataset = load_dataset(\"ncbi/Open-Patients\")\n",
    "print(dataset)\n",
    "print(\"Example record:\")\n",
    "first_split = list(dataset.keys())[0]\n",
    "print(dataset[first_split][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a871844",
   "metadata": {},
   "source": [
    "## Step 1.1 Clean and sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "from datasets import DatasetDict\n",
    "import numpy as np\n",
    "\n",
    "TEXT_FIELD_CANDIDATES = [\"text\", \"content\", \"description\", \"body\", \"note\"]\n",
    "sample_split = list(dataset.keys())[0]\n",
    "sample_item = dataset[sample_split][0]\n",
    "text_field = None\n",
    "for k in TEXT_FIELD_CANDIDATES:\n",
    "    if k in sample_item and isinstance(sample_item[k], str):\n",
    "        text_field = k\n",
    "        break\n",
    "if text_field is None:\n",
    "    for k, v in sample_item.items():\n",
    "        if isinstance(v, str):\n",
    "            text_field = k\n",
    "            break\n",
    "print(\"Using text field:\", text_field)\n",
    "\n",
    "KEEP_ASCII_ONLY = False\n",
    "MIN_LEN_CHARS = 10\n",
    "MAX_LEN_CHARS = 50000\n",
    "\n",
    "def basic_clean(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    if KEEP_ASCII_ONLY:\n",
    "        s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return s\n",
    "\n",
    "def map_clean(example):\n",
    "    t = example.get(text_field, \"\")\n",
    "    t = basic_clean(t)\n",
    "    example[text_field] = t\n",
    "    example[\"len_chars\"] = len(t)\n",
    "    return example\n",
    "\n",
    "def is_valid(example):\n",
    "    ln = example[\"len_chars\"]\n",
    "    return (ln >= MIN_LEN_CHARS) and (ln <= MAX_LEN_CHARS)\n",
    "\n",
    "cleaned = DatasetDict()\n",
    "for split in dataset.keys():\n",
    "    cleaned_split = dataset[split].map(map_clean, desc=f\"Cleaning {split}\")\n",
    "    cleaned_split = cleaned_split.filter(is_valid, desc=f\"Filtering {split}\")\n",
    "    cleaned[split] = cleaned_split\n",
    "\n",
    "def dedupe_exact(ds, key):\n",
    "    seen = set()\n",
    "    idxs = []\n",
    "    for i, s in enumerate(ds[key]):\n",
    "        if s not in seen:\n",
    "            idxs.append(i)\n",
    "            seen.add(s)\n",
    "    return ds.select(idxs)\n",
    "\n",
    "for split in list(cleaned.keys()):\n",
    "    before = len(cleaned[split])\n",
    "    cleaned[split] = dedupe_exact(cleaned[split], text_field)\n",
    "    after = len(cleaned[split])\n",
    "    if after != before:\n",
    "        print(f\"Deduped {split}: {before} -> {after}\")\n",
    "\n",
    "for split in cleaned.keys():\n",
    "    arr = np.array(cleaned[split][\"len_chars\"])\n",
    "    if arr.size:\n",
    "        print(f\"{split}: n={arr.size} mean={arr.mean():.1f} p50={np.percentile(arr,50):.0f} \"\n",
    "              f\"p90={np.percentile(arr,90):.0f} p99={np.percentile(arr,99):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecba1fb",
   "metadata": {},
   "source": [
    "## Step 2. Initialize tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62228981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sample = cleaned['train'][0][text_field] if 'train' in cleaned else list(cleaned.values())[0][0][text_field]\n",
    "encoded = tokenizer(sample, truncation=True, max_length=128)\n",
    "print(\"Tokenized sample IDs (first 20):\", encoded['input_ids'][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38742368",
   "metadata": {},
   "source": [
    "## Step 3. Tokenize dataset and chunk for CLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from itertools import chain\n",
    "SEQ_LEN = 1024\n",
    "\n",
    "def tokenize_function(examples, text_key):\n",
    "    return tokenizer(examples[text_key], truncation=False)\n",
    "\n",
    "# Remove all original columns so only token arrays remain\n",
    "remove_cols = cleaned['train'].column_names\n",
    "tokenized = cleaned.map(\n",
    "    partial(tokenize_function, text_key=text_field),\n",
    "    batched=True,\n",
    "    remove_columns=remove_cols,\n",
    "    desc='Tokenizing',\n",
    ")\n",
    "\n",
    "# Sanity check: ensure tokenized has only token-array columns\n",
    "print(tokenized)\n",
    "batch = tokenized['train'][:2]\n",
    "for k, v in batch.items():\n",
    "    print(k, type(v), type(v[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70330336",
   "metadata": {},
   "source": [
    "## Step 4. Save tokenized dataset and preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17711751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# from google.colab import drive  # not needed locally\n",
    "# drive.mount('./drive')  # not needed locally\n",
    "\n",
    "def group_texts(examples):\n",
    "    valid_keys = [k for k, v in examples.items() if isinstance(v, list) and v and isinstance(v[0], list)]\n",
    "    concatenated = {k: list(chain.from_iterable(examples[k])) for k in valid_keys}\n",
    "    total_length = len(concatenated['input_ids'])\n",
    "    total_length = (total_length // SEQ_LEN) * SEQ_LEN\n",
    "    result = {}\n",
    "    for k, t in concatenated.items():\n",
    "        result[k] = [t[i:i+SEQ_LEN] for i in range(0, total_length, SEQ_LEN)]\n",
    "    result['labels'] = list(result['input_ids'])\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized.map(group_texts, batched=True, desc='Grouping into fixed-length chunks')\n",
    "print(lm_datasets)\n",
    "\n",
    "OUT_DIR = \"./lab3_tokenized\"\n",
    "lm_datasets.save_to_disk(OUT_DIR)\n",
    "print(\"Saved:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
