{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Algocrat/slm-dragon-labs/blob/main/Labs/lab1_colab_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cee2fd2",
      "metadata": {
        "id": "2cee2fd2"
      },
      "source": [
        "\n",
        "# Lab 1 — Google Colab Setup for LoRA + Unsloth (SLM Dragon Trainer)\n",
        "\n",
        "This notebook prepares a clean, reliable Colab environment for fine‑tuning Small Language Models (SLMs) with **LoRA + Unsloth** on a GPU runtime.\n",
        "\n",
        "**What this does**\n",
        "- Verifies GPU\n",
        "- Installs a *matching* PyTorch trio (torch, torchvision, torchaudio) with auto‑fallback across CUDA wheels\n",
        "- Installs Unsloth + Unsloth Zoo and other LLM libs **without** breaking Torch deps\n",
        "- Loads a base model and runs a quick inference smoke test\n",
        "\n",
        "> If a later cell or library upgrades Torch and breaks compatibility, just re‑run the **Install PyTorch** and **Install LLM libs** cells.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f08e4a35",
      "metadata": {
        "id": "f08e4a35"
      },
      "source": [
        "\n",
        "## Step 0 — Enable GPU in Colab\n",
        "- Runtime → **Change runtime type** → Hardware accelerator: **GPU** → *Save*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1fbed5a",
      "metadata": {
        "id": "c1fbed5a"
      },
      "source": [
        "## Step 1 — Verify GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d4af170a",
      "metadata": {
        "id": "d4af170a",
        "outputId": "824bd85d-ff92-4316-ce87-3c90fccbc30f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "No GPU detected. Enable GPU under Runtime → Change runtime type.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!nvidia-smi || echo \"No GPU detected. Enable GPU under Runtime → Change runtime type.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8597c74",
      "metadata": {
        "id": "a8597c74"
      },
      "source": [
        "\n",
        "## Step 2 — Install a matched PyTorch build (auto‑fallback)\n",
        "This cell:\n",
        "- Removes conflicting stacks\n",
        "- Tries official wheels in order: **cu124 → cu121 → cu118**\n",
        "- Verifies import and CUDA availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c26a1a9",
      "metadata": {
        "id": "5c26a1a9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Clean potential conflicts\n",
        "!pip -q uninstall -y torch torchvision torchaudio fastai\n",
        "!pip -q cache purge\n",
        "\n",
        "def try_tag(tag):\n",
        "    print(f\"\\nTrying {tag} wheels...\")\n",
        "    # Use the official index for speed and correctness\n",
        "    rc = !pip install --no-cache-dir --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/{tag}\n",
        "    try:\n",
        "        import torch\n",
        "        print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"Torch import failed:\", e)\n",
        "        return False\n",
        "\n",
        "ok = False\n",
        "for tag in [\"cu124\", \"cu121\", \"cu118\"]:\n",
        "    if try_tag(tag):\n",
        "        ok = True\n",
        "        break\n",
        "\n",
        "if not ok:\n",
        "    raise SystemExit(\"Could not install a matching torch build. Make sure GPU is enabled, then rerun this cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "596d4daa",
      "metadata": {
        "id": "596d4daa"
      },
      "source": [
        "\n",
        "## Step 3 — Install LLM libraries (without touching Torch)\n",
        "We use `--no-deps` to prevent accidental Torch upgrades by pip.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "715c034e",
      "metadata": {
        "id": "715c034e"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -U unsloth unsloth_zoo accelerate transformers peft datasets bitsandbytes sentencepiece --no-deps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd18c80",
      "metadata": {
        "id": "1dd18c80"
      },
      "source": [
        "## Step 4 — Sanity check Torch + GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f696b91",
      "metadata": {
        "id": "1f696b91"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51497740",
      "metadata": {
        "id": "51497740"
      },
      "source": [
        "\n",
        "## Step 5 — Load a base model (Unsloth)\n",
        "> If you hit permission errors with Llama‑2, switch to the permissive Mistral model line below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601be27f",
      "metadata": {
        "id": "601be27f"
      },
      "outputs": [],
      "source": [
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = torch.float16\n",
        "\n",
        "# Choose one:\n",
        "# model_name = \"unsloth/llama-2-7b-bf16\"        # requires HF access\n",
        "model_name = \"unsloth/mistral-7b-v0.2-bf16\"     # permissive fallback\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "print(\"Model loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86c51c44",
      "metadata": {
        "id": "86c51c44"
      },
      "source": [
        "## Step 6 — Quick inference smoke test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f158ff",
      "metadata": {
        "id": "53f158ff"
      },
      "outputs": [],
      "source": [
        "\n",
        "prompt = \"What is the capital of France?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=32)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7f1e8a",
      "metadata": {
        "id": "6e7f1e8a"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "- **No GPU detected**: Enable GPU (Step 0) and **Runtime → Restart runtime**, then re‑run from Step 1.\n",
        "- **Dependency conflicts**: If a later install upgrades Torch, re‑run **Step 2** then **Step 3**.\n",
        "- **`ImportError: install unsloth_zoo`**: Re‑run **Step 3** to install `unsloth_zoo`.\n",
        "- **`pip` resolver warnings**: These are expected in Colab’s mixed environment. We explicitly pin Torch trio in Step 2 and use `--no-deps` in Step 3 to avoid breakage.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}