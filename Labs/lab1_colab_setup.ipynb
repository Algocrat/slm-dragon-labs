{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2cee2fd2",
      "metadata": {
        "id": "2cee2fd2"
      },
      "source": [
        "\n",
        "# Lab 1 — Google Colab Setup for LoRA + Unsloth (SLM Dragon Trainer)\n",
        "\n",
        "This notebook prepares a clean, reliable Colab environment for fine‑tuning Small Language Models (SLMs) with **LoRA + Unsloth** on a GPU runtime.\n",
        "\n",
        "**What this does**\n",
        "- Verifies GPU\n",
        "- Installs a *matching* PyTorch trio (torch, torchvision, torchaudio) with auto‑fallback across CUDA wheels\n",
        "- Installs Unsloth + Unsloth Zoo and other LLM libs **without** breaking Torch deps\n",
        "- Loads a base model and runs a quick inference smoke test\n",
        "\n",
        "> If a later cell or library upgrades Torch and breaks compatibility, just re‑run the **Install PyTorch** and **Install LLM libs** cells.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f08e4a35",
      "metadata": {
        "id": "f08e4a35"
      },
      "source": [
        "\n",
        "## Step 0 — Enable GPU in Colab\n",
        "- Runtime → **Change runtime type** → Hardware accelerator: **GPU** → *Save*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1fbed5a",
      "metadata": {
        "id": "c1fbed5a"
      },
      "source": [
        "## Step 1 — Verify GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d4af170a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4af170a",
        "outputId": "22acb9a2-8b38-4dec-a5c2-930009a70bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 11 13:57:01 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!nvidia-smi || echo \"No GPU detected. Enable GPU under Runtime → Change runtime type.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8597c74",
      "metadata": {
        "id": "a8597c74"
      },
      "source": [
        "\n",
        "## Step 2 — Install a matched PyTorch build (auto‑fallback)\n",
        "This cell:\n",
        "- Removes conflicting stacks\n",
        "- Tries official wheels in order: **cu124 → cu121 → cu118**\n",
        "- Verifies import and CUDA availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5c26a1a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c26a1a9",
        "outputId": "5e5004a2-16ee-4436-b0a2-c77334bf89f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "Trying cu124 wheels...\n",
            "Torch: 2.6.0+cu124 | CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Clean potential conflicts\n",
        "!pip -q uninstall -y torch torchvision torchaudio fastai\n",
        "!pip -q cache purge\n",
        "\n",
        "def try_tag(tag):\n",
        "    print(f\"\\nTrying {tag} wheels...\")\n",
        "    # Use the official index for speed and correctness\n",
        "    rc = !pip install --no-cache-dir --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/{tag}\n",
        "    try:\n",
        "        import torch\n",
        "        print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"Torch import failed:\", e)\n",
        "        return False\n",
        "\n",
        "ok = False\n",
        "for tag in [\"cu124\", \"cu121\", \"cu118\"]:\n",
        "    if try_tag(tag):\n",
        "        ok = True\n",
        "        break\n",
        "\n",
        "if not ok:\n",
        "    raise SystemExit(\"Could not install a matching torch build. Make sure GPU is enabled, then rerun this cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "596d4daa",
      "metadata": {
        "id": "596d4daa"
      },
      "source": [
        "\n",
        "## Step 3 — Install LLM libraries (without touching Torch)\n",
        "We use `--no-deps` to prevent accidental Torch upgrades by pip.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "715c034e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "715c034e",
        "outputId": "e40d8a12-521b-481f-8d39-535857567521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.8.4)\n",
            "Requirement already satisfied: unsloth_zoo in /usr/local/lib/python3.11/dist-packages (2025.8.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trl\n",
            "Successfully installed trl-0.21.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 3 — Install LLM libraries (without touching Torch)\n",
        "!pip install -U unsloth unsloth_zoo accelerate transformers peft datasets bitsandbytes sentencepiece trl --no-deps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd18c80",
      "metadata": {
        "id": "1dd18c80"
      },
      "source": [
        "## Step 4 — Sanity check Torch + GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1f696b91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f696b91",
        "outputId": "de7b2184-04ce-4158-f69f-d9819af0aeba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "Device: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51497740",
      "metadata": {
        "id": "51497740"
      },
      "source": [
        "\n",
        "## Step 5 — Load a base model (Unsloth)\n",
        "> If you hit permission errors with Llama‑2, switch to the permissive Mistral model line below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "601be27f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "601be27f",
        "outputId": "dd5b89ad-37cc-4236-c67b-54923361d2ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.8.4: Fast Mistral patching. Transformers: 4.55.0.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:37: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: unsloth/mistral-7b-v0.2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model_name = \"unsloth/mistral-7b-v0.2\"  # public, Unsloth-hosted\n",
        "# Source exists: https://huggingface.co/unsloth/mistral-7b-v0.2\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=2048,\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "print(\"Loaded:\", model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86c51c44",
      "metadata": {
        "id": "86c51c44"
      },
      "source": [
        "## Step 6 — Quick inference smoke test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "53f158ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53f158ff",
        "outputId": "a047737a-7f59-4960-d654-436588129e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the capital of France?\n",
            "\n",
            "Paris is the capital of France.\n",
            "\n",
            "What is the capital of France?\n",
            "\n",
            "Paris is the capital of France.\n",
            "\n",
            "What\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = \"What is the capital of France?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=32)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7f1e8a",
      "metadata": {
        "id": "6e7f1e8a"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "- **No GPU detected**: Enable GPU (Step 0) and **Runtime → Restart runtime**, then re‑run from Step 1.\n",
        "- **Dependency conflicts**: If a later install upgrades Torch, re‑run **Step 2** then **Step 3**.\n",
        "- **`ImportError: install unsloth_zoo`**: Re‑run **Step 3** to install `unsloth_zoo`.\n",
        "- **`pip` resolver warnings**: These are expected in Colab’s mixed environment. We explicitly pin Torch trio in Step 2 and use `--no-deps` in Step 3 to avoid breakage.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}